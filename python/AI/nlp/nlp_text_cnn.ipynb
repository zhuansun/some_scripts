{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 99, 103)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = '说起鸿蒙系统，相信大家都非常熟悉了，这个系统自2019年推出，是华为被打压之后进行的反击。'\n",
    "s2 = '而预约23999元的MateBook Fold 非凡大师的人数，超过了13万，就按23999元来算，这里就是31亿多，合计就是18万人预约，如果最终这些电脑都成交了，相当于销售金额就超过了35亿元。'\n",
    "s3 = '为何这以多人预约鸿蒙电脑？一方面当然是因为华为的号召力，不管是MateBook Pro，还是MateBook Fold 非凡大师都是产品力非常棒的产品，自然有人买，特别是折叠屏电脑，更是创新十足，噱头十足。'\n",
    "len(s1),len(s2),len(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 53, 54)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer\n",
    "import jieba\n",
    "\n",
    "# <UNK> 默认填充符号，当遇到不认识的词时，使用<UNK>填充\n",
    "# <PAD> 填充符号，当句子长度不够时，使用<PAD>填充\n",
    "words = {\"<UNK>\", \"<PAD>\"}\n",
    "for sentence in [s1, s2, s3]:\n",
    "    words = words.union(set(jieba.lcut(sentence)))\n",
    "\n",
    "# 字典\n",
    "word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# 把每句话进行token化\n",
    "s1_idx = [word2idx.get(word, word2idx.get(\"<UNK>\")) for word in jieba.lcut(s1)]\n",
    "s2_idx = [word2idx.get(word, word2idx.get(\"<UNK>\")) for word in jieba.lcut(s2)]\n",
    "s3_idx = [word2idx.get(word, word2idx.get(\"<UNK>\")) for word in jieba.lcut(s3)]\n",
    "\n",
    "len(s1_idx),len(s2_idx),len(s3_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 53, 53)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在早期，文本处理可以使用CNN来进行，后来，又有人用RNN来处理，当然，现在这些都不怎么用了，已经被先进的Transformer架构所取代了。\n",
    "# CNN处理NLP任务\n",
    "\"\"\"\n",
    "CNN卷积网络\n",
    "是用来处理图像的\n",
    "现在用来处理文本有一些前提条件\n",
    "训练的时候需要把训练集文本处理成长度大小一致的文本，通过填充或者截断\n",
    "缺点就是会引入噪声和丢失信息\n",
    "\"\"\"\n",
    "# 长度处理成一致的：全部处理成长度=53\n",
    "s1_idx2 = s1_idx + (len(s2_idx) - len(s1_idx)) * [word2idx.get(\"<PAD>\")]\n",
    "s2_idx2 = s2_idx.copy()\n",
    "s3_idx2 = s3_idx[:len(s2_idx)]\n",
    "\n",
    "len(s1_idx2),len(s2_idx2),len(s3_idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 53])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转张量\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "s_vec = torch.tensor(data = [s1_idx2, s2_idx2, s3_idx2], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 53])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59, 22, 37,  9, 40, 66, 74, 18,  4, 16,  9, 47, 37, 48, 26, 56, 54,  9,\n",
       "         58,  3, 80, 72, 79, 31, 42, 14, 39,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "        [ 1, 55, 62, 68, 42, 50, 81, 41, 81, 67, 75, 42, 70,  9, 15, 16, 23, 44,\n",
       "          9, 57, 60, 62, 77,  2,  9, 28, 63, 27, 46,  9, 13, 63, 61,  0, 55,  9,\n",
       "         76, 78, 51, 30, 74, 29, 16,  9, 52, 43, 12, 57, 15, 16, 73, 49, 39],\n",
       "        [69, 10, 65, 20, 55, 22, 30, 25, 83, 82, 64,  3, 42, 38,  9, 21, 58, 50,\n",
       "         81, 17,  9, 53, 50, 81, 41, 81, 67, 75, 74, 58, 84, 32, 18, 19, 42, 84,\n",
       "          9, 71, 24,  6,  9,  5, 58, 33, 35, 30,  9, 11, 36, 45,  9, 34, 45]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len: 53, dict_len: 85, embedding_dim: 32\n",
      "vec.shape: torch.Size([3, 53, 32])\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "seq_len = len(s1_idx2)\n",
    "dict_len = len(word2idx)\n",
    "embedding_dim = 32\n",
    "print(f\"seq_len: {seq_len}, dict_len: {dict_len}, embedding_dim: {embedding_dim}\")\n",
    "# ont-hot编码\n",
    "# [1, 0, 0, ...]\n",
    "# [0, 1, 0, ...]\n",
    "# [0, 0, 1, ...]\n",
    "# 因为one-hot编码，占空间，稀疏，不包含语义信息等等，基本都不用了。所以都用Embedding\n",
    "\n",
    "# 看我们的3个句子，每个句子长度是53，字典的长度是85，shape就是 [3, 53, 85]\n",
    "# Embedding可以降维，比如降维到 32； 就是把上面的矩阵[3, 53, 85]乘以一个降维矩阵[85,32]，结果就是[3, 53, 32]\n",
    "# 这是从数学角度来看的，实际上Embedding都给我们处理好了，直接调用API就行\n",
    "embed = nn.Embedding(num_embeddings=dict_len, \n",
    "                     embedding_dim=embedding_dim,\n",
    "                     padding_idx=word2idx.get(\"<PAD>\"))\n",
    "\n",
    "# Embedding\n",
    "vec = embed(s_vec)\n",
    "\n",
    "print('vec.shape:', vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在早期，文本处理可以使用CNN来进行，后来，又有人用RNN来处理，当然，现在这些都不怎么用了，已经被先进的Transformer架构所取代了。\n",
    "# CNN处理NLP任务\n",
    "# 构建CNN卷积网络\n",
    "class Conv1Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # 这里用的是 1d 1维卷积\n",
    "        self.conv1d = nn.Conv1d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.bn = nn.BatchNorm1d(num_features=out_channels)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.bn(x)\n",
    "        x = torch.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dict_len, embedding_dim=256, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=dict_len, \n",
    "                                 embedding_dim=embedding_dim,\n",
    "                                 padding_idx=word2idx.get(\"<PAD>\"))\n",
    "        self.conv1 = Conv1Block(in_channels=embedding_dim,\n",
    "                               out_channels=2 * embedding_dim)\n",
    "        self.conv2 = Conv1Block(in_channels=2 * embedding_dim,\n",
    "                               out_channels=4 * embedding_dim)\n",
    "        self.conv3 = Conv1Block(in_channels=4 * embedding_dim,\n",
    "                               out_channels=8 * embedding_dim)\n",
    "        self.conv4 = Conv1Block(in_channels=8 * embedding_dim,\n",
    "                               out_channels=16 * embedding_dim)\n",
    "        self.mp = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # 这里的要计算一下\n",
    "        self.fc1 = nn.Linear(in_features=45056,\n",
    "                           out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512,\n",
    "                            out_features=n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 先进行Embedding\n",
    "        x = self.embed(x)\n",
    "        # 维度转换：因为卷积要求的输入是[N,C,L]，我们现在是[N,L,C]所以要经过permute转换\n",
    "        x = torch.permute(input=x, dims=(0, 2, 1))\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.mp(x)\n",
    "        # 拉平，也可以使用flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试模型\n",
    "# 创建模型:这里是mock的数据，并不是用我们之前处理的s1,s2,s3那三个句子\n",
    "model = Model(dict_len=5000, embedding_dim=256, n_classes=2)\n",
    "# 构建测试数据\n",
    "X = torch.randint(low=0, high=5001, size=(3, 46))\n",
    "# 训练\n",
    "y_pred = model(X)\n",
    "# 查看模型返回结果\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
