{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"我爱北京天安门!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '爱', '北京', '天安门']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词\n",
    "\n",
    "import jieba\n",
    "# cut方法返回的是一个迭代器； lcut返回的是一个列表\n",
    "words = jieba.lcut(\"我爱北京天安门\")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化，Embedding\n",
    "# 抽取时序特征\n",
    "# [batch_size, seq_len, embedding_dim]\n",
    "# - batch_size: 批次大小\n",
    "# - seq_len: 序列长度\n",
    "# - embedding_dim: 每个 token 的向量维度\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(16,64,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "入参解释：\n",
    "- input_size    输入的每个词向量维度：256\n",
    "- hidden_size   输出每个词的向量维度：512\n",
    "- num_layers=1  RNN的层数\n",
    "- nonlinearity='tanh'   内部的激活函数\n",
    "- bias=True 是否有偏置\n",
    "- batch_first=False 是否把批量维度放在第一个 [seq_len, batch_size, embedding_dim]\n",
    "- dropout=0.0   默认不启用\n",
    "- bidirectional=False   是否是双向RNN\n",
    "- device=None   存储设备\n",
    "- dtype=None    数据类型\n",
    "\"\"\"\n",
    "rnn = nn.RNN(input_size=256, \n",
    "             hidden_size=512,\n",
    "             num_layers=1,\n",
    "             batch_first=False,\n",
    "             bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换之前的shape torch.Size([16, 64, 256])\n",
      "转换之后的shape torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "# 上面经过Embedding之后的数据格式是 [batch_size, seq_len, embedding_size]\n",
    "# 但是RNN要求的格式树令是 [seq_len, batch_size, embedding_size]\n",
    "# 所以要进行一层转换\n",
    "X1 = torch.permute(input=X, dims=(1, 0, 2))\n",
    "print(f\"转换之前的shape {X.shape}\")\n",
    "print(f\"转换之后的shape {X1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入是 torch.Size([64, 16, 256])\n",
      "输出是 torch.Size([64, 16, 512])\n",
      "最后一个输出是 torch.Size([1, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "# 观察RNN的API定义\n",
    "# 输入是 ：Inputs: input, hx\n",
    "# 输出是： Outputs: output, h_n\n",
    "\n",
    "# 输出的结果包含两个部分，output是记录了每一步的输出；而h_n是记录了最后时刻的输出\n",
    "# 输入的结果也是两个，第二个参数如果没有可以不传，或者传0\n",
    "h0 = torch.zeros(1, 16, 512, dtype=torch.float)\n",
    "output, h_n = rnn(X1,h0)\n",
    "print(f\"输入是 {X1.shape}\")\n",
    "print(f\"输出是 {output.shape}\")\n",
    "print(f\"最后一个输出是 {h_n.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output的最后一个其实和hn是一样的 \n",
      "output tensor([[-0.3548, -0.0835,  0.0580,  ..., -0.7443,  0.4115,  0.6679],\n",
      "        [ 0.8222,  0.5622, -0.3938,  ..., -0.5744, -0.1881, -0.1546],\n",
      "        [-0.0710,  0.5078, -0.2589,  ..., -0.5885,  0.2836, -0.0554],\n",
      "        ...,\n",
      "        [-0.3088,  0.2612, -0.0703,  ..., -0.1197, -0.4710, -0.0349],\n",
      "        [ 0.4588, -0.2597, -0.2544,  ..., -0.4350, -0.0330,  0.2473],\n",
      "        [-0.3499,  0.2656,  0.1242,  ..., -0.0787, -0.3473,  0.0507]],\n",
      "       grad_fn=<SliceBackward0>)  \n",
      " hn tensor([[[-0.3548, -0.0835,  0.0580,  ..., -0.7443,  0.4115,  0.6679],\n",
      "         [ 0.8222,  0.5622, -0.3938,  ..., -0.5744, -0.1881, -0.1546],\n",
      "         [-0.0710,  0.5078, -0.2589,  ..., -0.5885,  0.2836, -0.0554],\n",
      "         ...,\n",
      "         [-0.3088,  0.2612, -0.0703,  ..., -0.1197, -0.4710, -0.0349],\n",
      "         [ 0.4588, -0.2597, -0.2544,  ..., -0.4350, -0.0330,  0.2473],\n",
      "         [-0.3499,  0.2656,  0.1242,  ..., -0.0787, -0.3473,  0.0507]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"output的最后一个其实和hn是一样的 \\noutput {output[-1,:,:]}  \\n hn {h_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_hh_l0: torch.Size([512, 512])\n",
      "weight_ih_l0: torch.Size([512, 256])\n",
      "bias_hh_l0: torch.Size([512])\n",
      "bias_ih_l0: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# RNN层数不同，这里的层数就不同\n",
    "print(f\"weight_hh_l0: {rnn.weight_hh_l0.shape}\")\n",
    "print(f\"weight_ih_l0: {rnn.weight_ih_l0.shape}\")\n",
    "print(f\"bias_hh_l0: {rnn.bias_hh_l0.shape}\")\n",
    "print(f\"bias_ih_l0: {rnn.bias_ih_l0.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入是 torch.Size([64, 16, 256])\n",
      "两层的时候，这里只有最后一层的输出是 torch.Size([64, 16, 512])\n",
      "两层的时候，这里只有最后一层的最后一个输出是 torch.Size([2, 16, 512])\n",
      "output的最后一个其实和hn是一样的 \n",
      "output tensor([[-0.0727,  0.2223, -0.1220,  ..., -0.0049, -0.3443,  0.1957],\n",
      "        [ 0.0110, -0.1342, -0.4635,  ...,  0.2775,  0.0511,  0.2856],\n",
      "        [ 0.4682, -0.1468, -0.2681,  ..., -0.4639, -0.1938,  0.0648],\n",
      "        ...,\n",
      "        [-0.1844, -0.2150, -0.5185,  ..., -0.2146,  0.0795, -0.2569],\n",
      "        [ 0.1286, -0.1892,  0.2991,  ...,  0.1855, -0.1356,  0.3463],\n",
      "        [-0.1526, -0.1138, -0.0942,  ...,  0.2970, -0.1958, -0.0390]],\n",
      "       grad_fn=<SliceBackward0>)  \n",
      " hn tensor([[[-0.4653, -0.1231,  0.4320,  ..., -0.0613, -0.5454, -0.3012],\n",
      "         [ 0.3392, -0.4254,  0.1615,  ..., -0.1905,  0.1031, -0.4940],\n",
      "         [-0.1764,  0.7190,  0.4638,  ..., -0.1483,  0.4278,  0.1346],\n",
      "         ...,\n",
      "         [-0.6177,  0.7016,  0.0532,  ..., -0.6200, -0.1635, -0.1561],\n",
      "         [-0.1854, -0.2854, -0.2756,  ...,  0.3003, -0.2917, -0.0071],\n",
      "         [ 0.4215, -0.3647, -0.3091,  ...,  0.1044,  0.6937, -0.3612]],\n",
      "\n",
      "        [[-0.0727,  0.2223, -0.1220,  ..., -0.0049, -0.3443,  0.1957],\n",
      "         [ 0.0110, -0.1342, -0.4635,  ...,  0.2775,  0.0511,  0.2856],\n",
      "         [ 0.4682, -0.1468, -0.2681,  ..., -0.4639, -0.1938,  0.0648],\n",
      "         ...,\n",
      "         [-0.1844, -0.2150, -0.5185,  ..., -0.2146,  0.0795, -0.2569],\n",
      "         [ 0.1286, -0.1892,  0.2991,  ...,  0.1855, -0.1356,  0.3463],\n",
      "         [-0.1526, -0.1138, -0.0942,  ...,  0.2970, -0.1958, -0.0390]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "测试两层神经网络\n",
    "\"\"\"\n",
    "rnn = nn.RNN(input_size=256, \n",
    "             hidden_size=512,\n",
    "             num_layers=2,\n",
    "             batch_first=False,\n",
    "             bidirectional=False)\n",
    "X1 = torch.permute(input=X, dims=(1, 0, 2))\n",
    "h0 = torch.zeros(2, 16, 512, dtype=torch.float)\n",
    "output, h_n = rnn(X1,h0)\n",
    "print(f\"输入是 {X1.shape}\")\n",
    "print(f\"两层的时候，这里只有最后一层的输出是 {output.shape}\")\n",
    "print(f\"两层的时候，这里只有最后一层的最后一个输出是 {h_n.shape}\")\n",
    "print(f\"output的最后一个其实和hn是一样的 \\noutput {output[-1,:,:]}  \\n hn {h_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_hh_l0 torch.Size([512, 512])\n",
      "rnn.weight_hh_l1 torch.Size([512, 512])\n",
      "rnn.weight_ih_l0 torch.Size([512, 256])\n",
      "rnn.weight_ih_l1 torch.Size([512, 512])\n",
      "rnn.bias_hh_l0 torch.Size([512])\n",
      "rnn.bias_hh_l1 torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# RNN层数不同，这里的层数就不同\n",
    "print(f\"rnn.weight_hh_l0 {rnn.weight_hh_l0.shape}\")\n",
    "print(f\"rnn.weight_hh_l1 {rnn.weight_hh_l1.shape}\")\n",
    "print(f\"rnn.weight_ih_l0 {rnn.weight_ih_l0.shape}\")\n",
    "print(f\"rnn.weight_ih_l1 {rnn.weight_ih_l1.shape}\")\n",
    "print(f\"rnn.bias_hh_l0 {rnn.bias_hh_l0.shape}\")\n",
    "print(f\"rnn.bias_hh_l1 {rnn.bias_hh_l1.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双向RNN\n",
    "# 是针对输入的数据，正向输入一次，反向输入一次"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
