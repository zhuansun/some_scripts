{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义网络：LeNet-5\n",
    "class LeNet5(nn.Module):\n",
    "    # 定义lenet5的网络结构\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化方法：\n",
    "        1、接收超参数\n",
    "        \"\"\"\n",
    "        # 初始化父类\n",
    "        super(LeNet5, self).__init__()\n",
    "        # 定义卷积层\n",
    "        self.conv2d1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
    "        # 定义亚采样层（池化层）\n",
    "        self.maxpool2d1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # 定义卷积层\n",
    "        self.conv2d2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        # 定义亚采样层（池化层）\n",
    "        self.maxpool2d2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # 三个全连接层\n",
    "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    # 正向传播\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d1(x)\n",
    "        x = self.maxpool2d1(x)\n",
    "        x = self.conv2d2(x)\n",
    "        x = self.maxpool2d2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打包数据\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义Compose转换过程\n",
    "trans = transforms.Compose(transforms=[\n",
    "    # 将图像缩放为 32*32 【因为LeNet的要求是32x32的，这里为了全完模拟LeNet，所以对数据做调整，在实际中，可以调整模型而不处理数据】\n",
    "    # transforms.Resize(size=(32, 32)), 这种是硬拉伸【不推荐】\n",
    "    # transforms.CenterCrop(size=(32, 32)), 这种是以中心为原点，直接裁剪指定的大小，如果裁大了，会给补偿黑色背景\n",
    "    transforms.Resize(size=(32, 32)),\n",
    "    # LeNet要求图像通道是1层，这里也是处理数据，不处理模型。在实际中，可以调整模型而不处理数据\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    # 这里标准化的时候，图像有3层，就要写三次, transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "    # 因为我们在上面通过 Grayscale 转了灰度，只有一层，所以这里只有一个\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "        自定义数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, root, trans):\n",
    "        self.root = root\n",
    "        self.trans = trans\n",
    "        self.img_files = []\n",
    "        self.img_labels = []\n",
    "        self.label2idx = {f\"G{idx}\": idx for idx in range(10)}\n",
    "        self.idx2label = {v: k for k, v in self.label2idx.items()}\n",
    "        self._get_file_info()\n",
    "    \n",
    "    def _get_file_info(self):\n",
    "        for label in os.listdir(self.root):\n",
    "            # print(label)\n",
    "            label_root = os.path.join(self.root, label)\n",
    "            # print(label_root)\n",
    "            for file in os.listdir(label_root):\n",
    "                file_path = os.path.join(label_root, file)\n",
    "                self.img_files.append(file_path)\n",
    "                self.img_labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            返回数据集中有多少数据\n",
    "        \"\"\"\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            读取其中一个样本\n",
    "        \"\"\"\n",
    "        # 图像地址\n",
    "        img_file = self.img_files[idx]\n",
    "        # 图像标签\n",
    "        img_label = self.img_labels[idx]\n",
    "\n",
    "        # 读取图像\n",
    "        # 这里除了可以使用 Image 读取，还可以使用matplotlib或者cv2\n",
    "        img = Image.open(img_file)\n",
    "        \n",
    "        # 将图像缩放为100*100\n",
    "        # transforms.Resize(size=(100, 100)), 这种是硬拉伸【不推荐】\n",
    "        # transforms.CenterCrop(size=(100, 100)), 这种是以中心为原点，直接裁剪指定的大小，如果裁大了，会给补偿黑色背景\n",
    "        max_side = max(img.size)\n",
    "        img = transforms.CenterCrop(size=(max_side, max_side))(img)\n",
    "\n",
    "        # 图像数据处理\n",
    "        img = self.trans(img)\n",
    "\n",
    "        # 标签转张量\n",
    "        label = torch.tensor(data=self.label2idx[img_label], dtype=torch.long)   \n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    数据打包\n",
    "\"\"\"\n",
    "train_dataset = ImageDataset(root=\"./gestures/train/\", trans=trans)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=12, shuffle=True)\n",
    "\n",
    "test_dataset = ImageDataset(root=\"./gestures/test/\", trans=trans)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    准备训练\n",
    "\"\"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LeNet5().to(device=device)\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    定义过程监控函数\n",
    "\"\"\"\n",
    "def get_acc(data_loader):\n",
    "    # 模型的 eval 模式\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in data_loader:\n",
    "                # 数据搬家\n",
    "                batch_X = batch_X.to(device=device)\n",
    "                batch_Y = batch_Y.to(device=device)\n",
    "                # 正向传播\n",
    "                y_pred = model(batch_X)\n",
    "                # 简单解析\n",
    "                y_pred = y_pred.argmax(dim=-1)\n",
    "                # 计算准确率\n",
    "                acc = (y_pred == batch_Y).to(dtype=torch.float32).mean().item()\n",
    "                accs.append(acc)\n",
    "        final_acc = torch.tensor(data=accs, dtype=torch.float32).mean().item()\n",
    "    return final_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    定义训练过程\n",
    "\"\"\"\n",
    "def train():\n",
    "    # 先观察一下自然概率\n",
    "    train_acc = get_acc(data_loader=train_dataloader)\n",
    "    test_acc = get_acc(data_loader=test_dataloader)\n",
    "    \n",
    "    print(f\"未训练之前，初始的状态：训练集准确率：{train_acc}, 测试集准确率：{test_acc}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 模型设置为训练模式\n",
    "        model.train()\n",
    "        for batch_idx, (batch_X, batch_Y) in enumerate(train_dataloader):\n",
    "            # 数据搬家\n",
    "            batch_X = batch_X.to(device=device)\n",
    "            batch_Y = batch_Y.to(device=device)\n",
    "            # 正向传播\n",
    "            y_pred = model(batch_X)\n",
    "            # 计算误差\n",
    "            loss = loss_fn(y_pred, batch_Y)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 优化一步\n",
    "            optimizer.step()\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "        # 每轮结束，做一次测试\n",
    "        train_acc = get_acc(data_loader=train_dataloader)\n",
    "        test_acc = get_acc(data_loader=test_dataloader)\n",
    "        print(f\"当前训练到第 {epoch+1} 轮，训练集准确率：{train_acc}, 测试集准确率：{test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未训练之前，初始的状态：训练集准确率：0.07374100387096405, 测试集准确率：0.06862745434045792\n",
      "当前训练到第 1 轮，训练集准确率：0.08872901648283005, 测试集准确率：0.08578430861234665\n",
      "当前训练到第 2 轮，训练集准确率：0.1025179997086525, 测试集准确率：0.11029411852359772\n",
      "当前训练到第 3 轮，训练集准确率：0.11870503425598145, 测试集准确率：0.11519607901573181\n",
      "当前训练到第 4 轮，训练集准确率：0.1324940174818039, 测试集准确率：0.12745098769664764\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 模型设置为训练模式\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (batch_X, batch_Y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# 数据搬家\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         batch_X \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m         batch_Y \u001b[38;5;241m=\u001b[39m batch_Y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m img_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels[idx]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 读取图像\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 这里除了可以使用 Image 读取，还可以使用matplotlib或者cv2\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 将图像缩放为100*100\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# transforms.Resize(size=(100, 100)), 这种是硬拉伸【不推荐】\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# transforms.CenterCrop(size=(100, 100)), 这种是以中心为原点，直接裁剪指定的大小，如果裁大了，会给补偿黑色背景\u001b[39;00m\n\u001b[1;32m     46\u001b[0m max_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(img\u001b[38;5;241m.\u001b[39msize)\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/PIL/Image.py:3551\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3548\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3551\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[1;32m   3554\u001b[0m     checked_formats \u001b[38;5;241m=\u001b[39m ID\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/PIL/Image.py:3539\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[1;32m   3538\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3539\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3540\u001b[0m     _decompression_bomb_check(im\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m   3541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:873\u001b[0m, in \u001b[0;36mjpeg_factory\u001b[0;34m(fp, filename)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjpeg_factory\u001b[39m(\n\u001b[1;32m    871\u001b[0m     fp: IO[\u001b[38;5;28mbytes\u001b[39m], filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    872\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JpegImageFile \u001b[38;5;241m|\u001b[39m MpoImageFile:\n\u001b[0;32m--> 873\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mJpegImageFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m         mpheader \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39m_getmp()\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/PIL/ImageFile.py:147\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;167;01mIndexError\u001b[39;00m,  \u001b[38;5;66;03m# end of data\u001b[39;00m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;167;01mTypeError\u001b[39;00m,  \u001b[38;5;66;03m# end of data (ord)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         struct\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    154\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(v) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mv\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/github/some_scripts/.venv/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:372\u001b[0m, in \u001b[0;36mJpegImageFile._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m MARKER:\n\u001b[0;32m--> 372\u001b[0m     name, description, handler \u001b[38;5;241m=\u001b[39m \u001b[43mMARKER\u001b[49m[i]\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m         handler(\u001b[38;5;28mself\u001b[39m, i)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
